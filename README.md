# DLSpecialization_DeepLearningIA
Contains Solutions and Notes for the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew NG on Coursera.

## Course 1: [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/)

- [Week 1](https://www.coursera.org/learn/neural-networks-deep-learning/home/week/1)

    - What is a Neural Network (NN) ? 
    - Why is deep learning taking off ? 
    - Supervised learning with NN
    - Geoffrey Hinton interview. 

- [Week 2](https://www.coursera.org/learn/neural-networks-deep-learning/home/week/2)

    - Logistic Regression NN
    - Binary Classification
    - Gradient descent
    - Computational graph 
    - Python and vectorization
    - Pieter Abbeel interview.

- [Week 3](https://www.coursera.org/learn/neural-networks-deep-learning/home/week/3)

    - NN representation
    - Computing a NN output
    - Vectorizing across multiple examples
    - Activation Functions 
    - Gradient descent for NN
    - Back propagation computation and intuition 
    - Random weights initialization
    - Ian Goodfellow interview. 

- [Week4](https://www.coursera.org/learn/neural-networks-deep-learning/home/week/4)

    - Deep L-layer NN
    - Deep representations
    - Forward and backward propagation 
    - Parameters vs Hyperparameters
    - Getting your matrix dimensions right
    - What does NN have to do with the brain ? 

#### [Certificate Of Completion](https://coursera.org/share/9198bf9e5641668612752b5cd17be8a2)



## Course 2: [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network/home)


- [Week 1](https://www.coursera.org/learn/deep-neural-network/home/week/1)

    - Train / Dev / Test sets 
    - Bias and Variance
    - Basic Recipe for machine learning (Idea -> Implementation -> Experiment )
    - Regularization
        - Dropout  
        - Weight decay
    - Normalizing inputs
    - Vanishing/Exploding gradients
    - Weight initialization for DNN
    - Gradient checking

- [Week 2](https://www.coursera.org/learn/deep-neural-network/home/week/2)

    - Mini-batch gradient descent 
    - Exponentially weighted averages and bias correction
    - Gradient descent with momentum
    - RMSprop
    - Adam optimization algorithm inputs
    - Learning rate decay
    - The problem of local minima

- [Week 3](https://www.coursera.org/learn/deep-neural-network/home/week/3)

    - Tuning process
    - Batch normalization
    - Softmax regression
    - DeepLearning frameworks
    - Gradient tape

#### [Certificate Of Completion](https://coursera.org/share/4fbdf56b633deffa6166b342350d4219)


## Course 3: [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-network/home)


- [Week 1](https://www.coursera.org/learn/deep-neural-network/home/week/1)

    - Computer vision 
    - Edge detection filters
    - Pooling layers
    - Strided convolutions
    - Why convolutions? 
    - CNN convolutions

- [Week 2](https://www.coursera.org/learn/deep-neural-network/home/week/2)

    - Residual NN
    - Inception NN
    - MobileNet
    - EfficientNet
    - Transfer learning 
    - Data agumentation
    - Pooling layers
    - Strided convolutions
    - Why convolutions? 
    - CNN convolutions

- [Week 3](https://www.coursera.org/learn/deep-neural-network/home/week/2)

    - Object localization
    - Bounding box predictions
    - Intersection over union
    - Anchor boxes
    - YOLO algorithm
    - Semantic segmentation U-Net
    - U-Net architecture intuition 


